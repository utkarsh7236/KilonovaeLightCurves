{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.dates as mdates\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import GPy\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from matplotlib.colors import ListedColormap\n",
    "import warnings\n",
    "import time\n",
    "from itertools import product\n",
    "from joblib import Parallel, delayed\n",
    "from operator import itemgetter\n",
    "import sncosmo\n",
    "import emcee\n",
    "import corner\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "\n",
    "mpl.rcParams['legend.frameon'] = False\n",
    "mpl.rcParams['figure.autolayout'] = True\n",
    "# mpl.rcParams['figure.dpi'] = 300\n",
    "# mpl.rcParams['axes.spines.right'] = False\n",
    "# mpl.rcParams['axes.spines.top'] = False\n",
    "\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.sans-serif\": [\"Helvetica\"]})\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Palatino\"],\n",
    "})\n",
    "\n",
    "\n",
    "def utkarshGrid():\n",
    "    plt.minorticks_on()\n",
    "    plt.grid(color='grey',\n",
    "             which='minor',\n",
    "             linestyle=\":\",\n",
    "             linewidth='0.1',\n",
    "             )\n",
    "    plt.grid(color='black',\n",
    "             which='major',\n",
    "             linestyle=\":\",\n",
    "             linewidth='0.1',\n",
    "             )\n",
    "    \n",
    "# set random number seed to ensure reproducibility\n",
    "seed = 3\n",
    "rstate = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing MCMC using GP Emulator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/utkarsh/PycharmProjects/KilonovaeLightCurves/Emulator'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.dirname(os.getcwd())\n",
    "os.chdir(path)\n",
    "from Emulator.Classes.AllData import AllData\n",
    "from Emulator.Classes.LightCurve import utkarshGrid, LightCurve\n",
    "from Emulator.Classes.GP import GP\n",
    "from Emulator.Classes.GP2D import GP2D\n",
    "from Emulator.Classes.GP5D import GP5D\n",
    "os.chdir(os.getcwd() + \"/Emulator\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_arr = np.array([0.01, 0.13, 60, 4]) #[mejdyn, mejwind, phi, iobs]\n",
    "mejdyn_guess, mejwind_guess, phi_guess, iobs_guess = [0.1, 0.1, 20, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fluxes(mejdyn = truth_arr[0], mejwind = truth_arr[1], phi = truth_arr[2], \n",
    "                 iobs = truth_arr[3], messages = False):\n",
    "    curr_wv = np.arange(100, 3600, 10)\n",
    "    set_skip_factor = None\n",
    "    gp = GP5D(\"Classes/reference.csv\")\n",
    "    gp.split = 1\n",
    "    gp.emulator = \"start\"\n",
    "    gp.cross_validation = (mejdyn, mejwind, phi, iobs)\n",
    "    gp.set_wv_range(curr_wv)\n",
    "    gp.n_comp = 25\n",
    "    time_shape = None\n",
    "    gp.save_pca_components(skip_factor = set_skip_factor)\n",
    "    gp.setXY_cross_validation(mejdyn, mejwind, phi, iobs, messages = False)\n",
    "    fitting_kernel = GPy.kern.RBF(input_dim=4, variance = 1, lengthscale=1, ARD = True)\n",
    "    decay_kernel = GPy.kern.Linear(input_dim=4, ARD = True)\n",
    "    gp.kernel = fitting_kernel * decay_kernel\n",
    "    gp.model = GPy.models.GPRegression(gp.X,gp.Y,gp.kernel)\n",
    "    if messages:\n",
    "        print(gp.model)\n",
    "    t0 = time.time()\n",
    "    if messages:\n",
    "        print(f\"[STATUS] Optimizing...\")\n",
    "    gp.model.optimize(messages = False)\n",
    "    gp.model_predict(include_like = True, messages = False)\n",
    "    gp.save_pca_initial_validation()\n",
    "    gp.delete_folder_files(\"data/pcaComponentsTrained\")\n",
    "    gp.delete_folder_files(\"data/pcaComponentsTrainedError\")\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = train_fluxes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fluxes(mejdyn, mejwind, phi, iobs, gp = gp, extra_item = None):\n",
    "    gp.validationX = [mejdyn, mejwind, phi, iobs]\n",
    "    theta = gp.validationX\n",
    "    gp.model_predict_cross_validation(include_like = True, messages = False) # Save cross validation\n",
    "    gp.save_trained_data(errors = False, theta = (mejdyn, mejwind, phi, iobs), extra_item = extra_item)\n",
    "    y = np.load(f\"data/pcaTrained/mejdyn{mejdyn}_mejwind{mejwind}_phi{phi}_iobs{iobs}.npy\")\n",
    "    t = gp._t_helper()\n",
    "    t_matrix = np.repeat(t, gp.num_wv).reshape(len(t), gp.num_wv)\n",
    "    x = t_matrix\n",
    "    os.remove(f\"data/pcaComponentsTrained/mejdyn{mejdyn}_mejwind{mejwind}_phi{phi}_iobs{iobs}.npy\")\n",
    "    os.remove(f\"data/pcaComponentsTrainedError/mejdyn{mejdyn}_mejwind{mejwind}_phi{phi}_iobs{iobs}.npy\")\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = predict_fluxes(mejdyn = truth_arr[0], mejwind = truth_arr[1], \n",
    "                      phi = truth_arr[2], iobs = truth_arr[3], extra_item = True)\n",
    "yerr = 0.2 * y\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(theta):\n",
    "    \"\"\"log-prior as a function of parameters `theta`.\"\"\" \n",
    "    mejdyn, mejwind, phi, iobs = theta\n",
    "    lim = 1e6\n",
    "    if mejdyn > 1 or mejdyn < 0.001:\n",
    "        return -np.inf\n",
    "    elif mejwind > 1 or mejwind < 0.001:\n",
    "        return -np.inf\n",
    "    elif phi > 91 or phi < 0:\n",
    "        return -np.inf\n",
    "    elif iobs > 11 or iobs < 0:\n",
    "        return -np.inf\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def loglike(theta, x, y, yerr):\n",
    "    \"\"\"log-likelihood as a function of parameters `theta`.\"\"\"\n",
    "    \n",
    "    mejdyn, mejwind, phi, iobs = theta\n",
    "    x, y_model = predict_fluxes(mejdyn, mejwind, phi, iobs)\n",
    "    logl = - 0.5 * np.sum(((y - y_model)/yerr)**2)\n",
    "    return logl\n",
    "\n",
    "def logpost(theta, x=x, y=y, yerr=yerr):\n",
    "    \"\"\"(Negative) log-posterior as a function of parameters `theta`.\"\"\"\n",
    "    if not np.isfinite(prior(theta)):\n",
    "        return -np.inf\n",
    "    mejdyn, mejwind, phi, iobs = theta  # reassign parameters\n",
    "    logp = prior(theta)  # prior\n",
    "    logl = loglike(theta, x=x, y=y, yerr = yerr)  # likelihood\n",
    "    return (logl + logp)  # posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "initial = [mejdyn_guess, mejwind_guess, phi_guess, iobs_guess]\n",
    "ndim = len(initial)\n",
    "nwalkers = 10\n",
    "nburn = 1\n",
    "niter = 10\n",
    "t_init = time.time()\n",
    "p0 = [np.array(initial) + 1 * np.random.randn(ndim) for i in range(nwalkers)]\n",
    "p0 = np.array(p0, dtype = float)\n",
    "print(p0.shape)\n",
    "\n",
    "pool = None\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, logpost, pool = pool)\n",
    "t0 = time.time()\n",
    "print(\"Started Burn-In\")\n",
    "state = sampler.run_mcmc(p0, nburn, progress=False)\n",
    "print(f\"Burn-In Took: {round(time.time() - t0)}s\")\n",
    "sampler.reset()\n",
    "state = sampler.run_mcmc(state, niter, progress=True)\n",
    "\n",
    "gp.delete_folder_files(\"data/pcaTrained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get final chains\n",
    "samples = sampler.get_chain()\n",
    "labs = ['mejdyn', 'mejwind', 'phi', \"iobs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot posterior\n",
    "corner.corner(samples.reshape(-1, ndim),  # collect samples into N x 3 array\n",
    "              bins=10,  # bins for histogram\n",
    "              show_titles=True, quantiles=[0.16, 0.84],  # show median and uncertainties\n",
    "              labels=labs,\n",
    "              truths=truth_arr,  # plot truth\n",
    "              color='darkviolet', truth_color='black',  # add some colors\n",
    "              **{'plot_datapoints': False, 'fill_contours': True});  # change some default options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a few chains\n",
    "plt.figure(dpi = 300, figsize = (10,8))\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "[plt.plot(samples[:, i, 0], alpha=0.5) for i in range(nwalkers)]\n",
    "plt.xlim([0, niter])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel(labs[0])\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "[plt.plot(samples[:, i, 1], alpha=0.5) for i in range(nwalkers)]\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel(labs[1])\n",
    "plt.xlim([0, niter])\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "[plt.plot(samples[:, i, 2], alpha=0.5) for i in range(nwalkers)]\n",
    "plt.xlim([0, niter])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel(labs[2])\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "[plt.plot(samples[:, i, 3], alpha=0.5) for i in range(nwalkers)]\n",
    "plt.xlim([0, niter])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel(labs[3])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_pow_two(n):\n",
    "    i = 1\n",
    "    while i < n:\n",
    "        i = i << 1\n",
    "    return i\n",
    "\n",
    "def auto_window(taus, c):\n",
    "    m = np.arange(len(taus)) < c * taus\n",
    "    if np.any(m):\n",
    "        return np.argmin(m)\n",
    "    return len(taus) - 1\n",
    "\n",
    "def autocorr_func_1d(x, norm=True):\n",
    "    x = np.atleast_1d(x)\n",
    "    if len(x.shape) != 1:\n",
    "        raise ValueError(\"invalid dimensions for 1D autocorrelation function\")\n",
    "    n = next_pow_two(len(x))\n",
    "\n",
    "    # Compute the FFT and then (from that) the auto-correlation function\n",
    "    f = np.fft.fft(x - np.mean(x), n=2 * n)\n",
    "    acf = np.fft.ifft(f * np.conjugate(f))[: len(x)].real\n",
    "    acf /= 4 * n\n",
    "\n",
    "    # Optionally normalize\n",
    "    if norm:\n",
    "        acf /= acf[0]\n",
    "\n",
    "    return acf\n",
    "\n",
    "# Following the suggestion from Goodman & Weare (2010)\n",
    "def autocorr_gw2010(y, c=5.0):\n",
    "    f = autocorr_func_1d(np.mean(y, axis=0))\n",
    "    taus = 2.0 * np.cumsum(f) - 1.0\n",
    "    window = auto_window(taus, c)\n",
    "    return taus[window]\n",
    "\n",
    "def autocorr_new(y, c=5.0):\n",
    "    f = np.zeros(y.shape[1])\n",
    "    for yy in y:\n",
    "        f += autocorr_func_1d(yy)\n",
    "    f /= len(y)\n",
    "    taus = 2.0 * np.cumsum(f) - 1.0\n",
    "    window = auto_window(taus, c)\n",
    "    return taus[window]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi= 300)\n",
    "for dimension in range(len(initial)):\n",
    "    chain = sampler.get_chain()[:, :, dimension].T\n",
    "    N = np.exp(np.linspace(np.log(10), np.log(chain.shape[1]), 20)).astype(int)\n",
    "\n",
    "    # Compute the estimators for a few different chain lengths\n",
    "    gw2010 = np.empty(len(N))\n",
    "    new = np.empty(len(N))\n",
    "    for i, n in enumerate(N):\n",
    "        gw2010[i] = autocorr_gw2010(chain[:, :n])\n",
    "#         new[i] = autocorr_new(chain[:, :n])\n",
    "\n",
    "    # Plot the comparisons\n",
    "    plt.loglog(N, gw2010, \"o-\", label=f\"{labs[dimension]}\")\n",
    "#     plt.loglog(N, new, \"o-\", label=f\"New: {labs[dimension]}\")\n",
    "    \n",
    "    \n",
    "ylim = plt.gca().get_ylim()\n",
    "plt.ylim(ylim)\n",
    "plt.xlabel(\"number of samples, $N$\")\n",
    "plt.ylabel(r\"$\\tau$ estimates\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.plot(N, N / 50.0, \"--k\", label=r\"$\\tau = N/50$\")     \n",
    "plt.title(f\"Autocorrelation\")\n",
    "print(f\"Walkers:{nwalkers}\\nIterations:{niter}\\nTotal Runtime:{round((time.time()-t_init)/60, 2)}min\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
